{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5\n",
    "\n",
    "source: [565_quiz2 google doc](https://docs.google.com/document/d/1YRZ3dWxVRcuXfEFPX1JDEULiV7HXogjqG95gxf8Aooo/edit)\n",
    "### Define an ordinal classification problem, and give an example\n",
    " * Definition: classification problem where the classes have some sort of order\n",
    "    * It is a type of regression analysis used for predicting an ordinal variable\n",
    "* Example: Amazon ratings / movie reviews / any opinions that are expressed on a scale\n",
    "\n",
    "### Give 3 possible solutions to the rating classification using standard ML models, and mention a disadvantage for each\n",
    "* Can use the probability associated with a binary classifier (predict_proba)\n",
    "    * But is it (the probability) really capturing intensity or just reliability?\n",
    "    * Unclear as to how to use neutral/ambivalent data (far from every class?)\n",
    "* Or do a 5-class classification problem\n",
    "    * Not taking advantage of the ordinal structure\n",
    "* Or change it into a regression problem\n",
    "    * Too fine-grained, wrong objective\n",
    "        * 1.1, 1.2, 0.9, etc are not acceptable as 1\n",
    "\n",
    "### Give the main idea of SVM-based ranking\n",
    "\n",
    "* Julian: If A is ranked higher than B, you can create a positive example for your SVM rank classifier by subtracting B's feature vector from A, or a negative example for your classifier by subtracting A from B. When you have trained your binary classifier and have a weight vector W, you can get a value which can be used to rank any new document C (relative to others) by taking the dot product of W and the feature vector for C.\n",
    "\n",
    "* Pairwise intuition\n",
    "    * With ratings scales, reviews with 1 starts should be ranked lower than those with 2, 3, 4, and 5.\n",
    "    * For all document d_0, d_1, …, d_N in D, if rating(d_i) > rating(d_k), output(d_i) > output(d_j) \n",
    "    * If we use a linear model, “output” is of the form w * pi(d_i) where pi() generates a feature vector for a document, so we can express our objective output(d_i) > output(d_j) as \n",
    "        * w* pi(d_i) > w*pi(d_j) \n",
    "        * = w*(pi(d_i) - pi(d_j)) > 0\n",
    "\t\n",
    "    Which is a binary classification problem with new features vectors corresponding to pairwise differences of the original. \n",
    "    \n",
    "        * If rating(d_i) > rating(d_k), result : positive\n",
    "        * If rating(d_i) < rating(d_k) result : negative ⇒ binary classification\n",
    "\n",
    "\n",
    "### SVM ranking\n",
    "\n",
    "* SVM is a linear model (by default) and ideal for this kind of ranking problem\n",
    "* It can handle huge set of training examples and a huge feature space\n",
    "* We don’t need the output of `predict` (which gives positive or negative), just calculate dot products directly.\n",
    "    * X_test.dot(svc.coef_[0]) # feature vector.dot(svc coefficients)\n",
    "* This is useful for information retrieval as well\n",
    "\n",
    "### Know how to convert feature vectors and ratings to prepare for SVM Rank\n",
    "\n",
    "* For each datapoint, randomly select one other datapoint that has a different rating.\n",
    "* Create a new feature vector which is the difference between the two data points(difference of feature vectors)\n",
    "* Create a label which is 1 if the rating of the first one is larger, 0 if the second is larger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pairwise(data,ratings):\n",
    "    pairwise_data = []\n",
    "    pairwise_class = []\n",
    "  \n",
    "    for i in range(data.shape[0]):\n",
    "        j = random.randint(0, data.shape[0]-1)\n",
    "        \n",
    "        # if the ratings are the same, do it again\n",
    "        while ratings[i] == ratings[j]:\n",
    "            j = random.randint(0, data.shape[0]-1)\n",
    "        \n",
    "        # stack diff between datapoints\n",
    "        diff_data = data[i] - data[j]\n",
    "        pairwise_data.append(diff_data)\n",
    "        \n",
    "        # add classes\n",
    "        if ratings[i] > ratings[j]:\n",
    "            pairwise_class.append(1)\n",
    "        else:\n",
    "            pairwise_class.append(0)\n",
    "\n",
    "    output_pairwise_data = vstack(pairwise_data)\n",
    "    return output_pairwise_data, pairwise_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Know how to calculate the score for a text after the SVM ranking model has been built\n",
    "* Train a SVC model with the pairwise data & pairwise class.\n",
    "* Dot product a feature vector with the coefficients of the SVC model trained on the pairwise data and labels\n",
    "* Calculate Kendall’s Tau with predictions and the gold standard\n",
    "\n",
    "### Given a pair of rankings (one gold standard, one predicted), calculate Kendall's Tau\n",
    "(# of concordant pairs - # discordant pairs) / total # of pairs\n",
    "A concordant pair = a pair where the order is the same\n",
    "example\n",
    "X1 = [1, 2, 3, 4, 5, 6, 7]\n",
    "X2 = [1, 3, 2, 4, 5, 7, 6]\n",
    "   * Concordant = 19\n",
    "   * Discordant = 2\n",
    "   * Total = 21\n",
    "   * = (19-2) / 21 = 0.809\n",
    "\n",
    "### Give one major problem with training a fake review classifier\n",
    "* Fake fakes are a lot different from real fakes so they may not be as accurate\n",
    "* Very difficult to get “the gold standard”\n",
    "* Difficult to identify fake reviews too\n",
    "* Generally metadata is much more promising than text for identifying fakes\n",
    "\n",
    "### Define argumentation mining relative to other CL topics such as sentiment analysis and discourse\n",
    "\n",
    "* The identification and segmentation of argumentative units\n",
    "* The identification and classification of supporting and objecting units\n",
    "* The identification and classification of argumentative structure\n",
    "    * We could make use of these argumentation mining to analyze sentiments, polarities, etc of a text.\n",
    "\n",
    "### Know the major indicators of sarcasm, and be able to identify instances using those indicators\n",
    "* Exaggerations and hyperbole\n",
    "* Use of interjections\n",
    "* Incongruent punctuation\n",
    "* Striking polarity conflict (hashtags / emojis that indicate the opposite sentiment of the content of the tweet)\n",
    "* Obvious role-playing\n",
    "* Fixed expressions (yeah, not so much)\n",
    "* Explicit mention of sarcasm (#sarcasm)\n",
    "\n",
    "### Know what information beyond an individual tweet/post is important for identifying sarcasm\n",
    "* User profiling \n",
    "* Identify user stance toward relevant targets\n",
    "* Identify user personality as determined by entire tweet history\n",
    "* Identify user mood as determined by immediate tweet history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6\n",
    "\n",
    "### Provide a basic schema for emotion classification (i.e. list 5/6/7 major emotions)\n",
    "Surprise, happiness, anger, fear, disgust, and sadness, etc\n",
    "\n",
    "### Define distant supervision\n",
    "* Finding features that allow you to find labels that may not be perfectly accurate but accurate enough that you could train on\n",
    "* We can build a classification problem where the training set comes from sources\n",
    "* Use some information (such as hashtags, emojis) in a tweet as a “label” not a “feature”\n",
    "\n",
    "### Explain how an emotion classification dataset can be derived using distant supervision\n",
    "* Features on social media such as:\n",
    "* Hashtags e.g. #Depressed\n",
    "* Emoji  \n",
    "\n",
    "### Know what is meant by the \"big five\" personality traits \n",
    "* “Big five” → five major human personalities\n",
    "* Extroversion (vs. Introversion)\n",
    "* Emotional stability (vs. Neuroticism)\n",
    "* Agreeableness (vs. Disagreeableness)\n",
    "* Conscientiousness (vs. Unconscientiousness)\n",
    "* Openness (vs. Conservatism)  \n",
    "                 \n",
    "### Distinguish between an open vocabulary and a closed vocabulary approach → Schwatz\n",
    "\n",
    "* \"Closed vocabulary\" using LIWC (content analysis)\n",
    "    * Closed vocabulary approach means that using a lexicon which has predefined categories for each word.\n",
    "    *  The latest version of LIWC has 64 categories such as articles, prepositions, … family, affect, body, etc.\n",
    "    *  Something like:\n",
    "        *  Neurotic and agreeable people tend to use more first-person singulars.\n",
    "        * People low in openness talk more about social processes.\n",
    "        *  Females use more first person singular pronouns, males use more articles.\n",
    "    *  Most studies linking language with psychological variables rely on a priori fixed sets of words, such as the LIWC categories carefully constructed over 20 years of human research.\n",
    "    *  When one has a specific theory in mind or a small sample size, this can be ideal.\n",
    "\n",
    "\n",
    "* Open vocabulary\n",
    "    * The words and clusters of words analyzed are determined by the data itself\n",
    "        * Enable us to find correlations such as: the word ‘hug’ is positively correlated with “agreeableness” (which is not categorized in LIWC)\n",
    "        * Extract words, phrases, and topics (automatically clustered sets of words) from millions of data, and find the language that correlates most with gender, age, and five factors or personality.\n",
    "    * User-normalized frequency of words and phrases identified using PMI\n",
    "    * LDA topic models\n",
    "         * Build a topic model for the entire corpus\n",
    "         * Calculate a probability of topic for a given subject p(t|s)\n",
    "    *  Does not rely on predefined categories of each word, rather, it draws characteristics of word usages using PMI and LDA. \n",
    "    * Open vocabulary analysis yields further insights into the behavioral residue of personality types beyond those from a priori word-category based approaches, giving unanticipated results (correlations between unexpected category of linguistic features (e.g. Japanese cartoons, sports teams, etc) and personality, gender, or age).\n",
    "\n",
    "* Models created with open vocabulary features outperformed those created based on LIWC features.\n",
    "* A model which includes LIWC features on top of the open-vocabulary features does not result in any improvement suggesting that the open-vocabulary features are able to capture predictive information which fully supersedes LIWC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Know the common computational linguistics techniques used for linguistic feature extraction in the [Schwatz et al study](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0073791) [draw connections from paper to what we learned in class] [PMI and Topic modelling] \n",
    "* Emoticon-aware word tokenization \n",
    "* n-grams\n",
    "* Lexical approaches\n",
    "* Words and phrases extractions\n",
    "    * N-grams\n",
    "    * Extract phrases which have high informative value according to PMI (pointwise mutual information), a ratio of the joint-probability to the independent probability of observing the phrase. (keep words and phrases which are commonly used)\n",
    "* Topic extractions\n",
    "    * LDA: topics are a distribution of words\n",
    "    * Produced 2000 topics in the paper\n",
    "* Word Cloud visualization\n",
    "    * Summarize results \n",
    "    * Represent distinguishing topics \n",
    "\n",
    "### Identify which are the features (independent variables) and which are the output/response/dependent variables for the correlation analysis in Schwartz et al, and contrast this with a \"typical\" machine learning technique in author profiling\n",
    "\n",
    "* Using personality/age/gender as the input to predict the use of words instead of the other way around (usually words → predict personality, age, gender)\n",
    "    * Including multiple variables in a single model helps eliminate spurious correlation\n",
    "    * “Our correlational analysis produces a comprehensive list of the most distinguishing language features for any given attribute, words, phrases, or topics which maximally discriminate a given target variable.”\n",
    "\n",
    "* Word/phrase/topic probability are the output/response/dependent variables\n",
    "    * A different linear regression model for each word/phrase/topic\n",
    "    * Correct for multiple tests (Bonferroni corrected significance levels)\n",
    "\n",
    "* For a given word/phrase/topic, the coefficient associated with personality/age/gender for its model is its \"correlation\"\n",
    "* Correlation analysis was conducted for distinguishing individual effects of each feature such as age, gender, personality. \n",
    "\n",
    "\n",
    "\n",
    "### With reference to the results of Schwartz at al, discuss how \"traditional\" sentiment analysis (i.e. positive and negative) relates to variation due to personality, ages, and gender discovered in that study.\n",
    "\n",
    "* Used a lot of samples and kept only the significant features to drive results (vs. traditional study had limited sizes of samples -- which led to false discovery such as males use more emoticons)\n",
    "* Ages \n",
    "    * Youngest: slang, emoticons, internet speak, school\n",
    "    * 23-29: A couple internet speak, work appearing, beer, etc\n",
    "    * ‘I’ decreases after age 22 → ‘we’\n",
    "    * Limitation: rarity of older individuals\n",
    "\n",
    "* Gender\n",
    "    * Females use more psychological and social processes, emotion words, first-person singulars.\n",
    "    * Males use more possessive words (my wife, my girlfriend)\n",
    "\n",
    "* Personality\n",
    "    * Based on a big five questionnaire\n",
    "    * Extroverts: party, love you, boys, ladies vs. introverts: computer, internet, reading, anime, manga, japanese, etc\n",
    "    * Openness: music, art, writing, dream, universe, soul\n",
    "    * Some features were extracted which LIWC did not capture\n",
    "    * Neuroticism: sick of  (negative words?)\n",
    "    * Language related to emotional stability (low neuroticism)\n",
    "        * Emotional stability → sports, vacation, beach, church, team, family time (active words, positive words related to emotional stability?)\n",
    "\n",
    "\n",
    "\n",
    "# Lecture 7\n",
    "\n",
    "### Know what information a geoJSON file contains (how is the map information stored?).\n",
    "\n",
    "* Name, id, created_at, updated_at, geometry(Multipolygon object which is bounded by a bunch of points)\n",
    "* Multipolygon method: centroid (return a point object), contains(returns a boolean)\n",
    "\n",
    "### Identify situations where we might want to manipulate marker size when plotting points on a map\n",
    "* Marker size to reflect the data you have (e.g. population, crime rate etc)\n",
    "* Individual vs region\n",
    "\n",
    "### Define a choropleth and distinguish it from simple plotting of points on maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "fig = px.choropleth(\n",
    "    Prov_happy_df,  # identifiers in the geoJSON and the df must be aligned(“name”)\n",
    "    geojson=ca_geojson,\n",
    "    color=\"happiness\", # column whose value will be used for colors\n",
    "    locations=\"name\", # location names\n",
    "    projection=\"mercator\", # a projection of the map to improve the look\n",
    "    color_continuous_scale=\"sunset\", # color scheme\n",
    "    hover_data=[\"happiness\", \"name\"], # a list of columns which should appear when hovering over\n",
    ")\n",
    "fig.update_geos(fitbounds=\"locations\") # centre the plot to the relevant parts of the world\n",
    "fig.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Points: visualize individual points and their properties\n",
    "* Choropleth: visualize regions and their properties\n",
    "    * A map which is divided up into administrative regions with each region given a color based on some variable.\n",
    "\n",
    "### Contrast the three major classes in the datetime module\n",
    "\n",
    "* Datetime: Date + time  (indicate an exact moment in time)\n",
    "\t`datetime.datetime(2020,3,16,6,30,1)`\n",
    "    \n",
    "\tMethods: ctime()\n",
    "\n",
    "* Date: only date \n",
    "\t`datetime.date(2020,3,16)`\n",
    "\n",
    "\tMethods: replace(year=,month=,day=)\n",
    "\n",
    "* Time: only time\n",
    "\t `datetime.time(6,30,1)`\n",
    "\n",
    "\t Methods: replace(hour=,minute=,second=)\n",
    "\n",
    "### Explain how timezones work in the datetime module\n",
    "* Switch the timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "from pytz import timezone\n",
    "\n",
    "datetime_string1 = \"9:30 PM, Mar 16, 2021\"\n",
    "format_string1 =\"%I:%M %p, %b %d, %Y\"\n",
    "dt = datetime.datetime.strptime(datetime_string1,format_string1)\n",
    "\n",
    "pacific_time = timezone(\"Canada/Pacific\")\n",
    "localized_dt = pacific_time.localize(dt)    # convert native time to local time\n",
    "\n",
    "localized_dt.astimezone(timezone('Canada/Eastern')) # convert local time to native time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8\n",
    "\n",
    "### Define code-switching and explain why code-switching is a challenge for NLP\n",
    "* CS is switching between languages in the same sentence\n",
    "* It’s challenging for NLP because:\n",
    "    * OOV words if it’s not in our monolingual corpora\n",
    "        * Even if it’s in the corpora, it may be using the word in a different context, which makes it difficult to build good embeddings\n",
    "    * Hard to identify syntax structure\n",
    "    * Encoding problems with different characters \n",
    "* Two languages in one sentence; no word2vec, OOV problem, ...etc \n",
    "\n",
    "### Know that emoji have a corresponding text string, and how it can be used for sentiment analysis\n",
    "* 🤐 :zipper-mouth_face:\n",
    "* This can give good sentiment because the emoji can now be encoded in the BOW or whatever sentiment classifier. \n",
    "\n",
    "### Compare and contrast emoji and emoticons in terms of their computational representation and distribution in corpora.\n",
    "* Both usually:\n",
    "    * Appear at the end of sentence (substitutes punctuations)\n",
    "    * Can be used multiple time for intensification\n",
    "    * Usually redundant or are usually nouns\n",
    "    * Express emotions\n",
    "* Differences:\n",
    "    * Emoticons are created from standard ascii punctuation instead of having their own unicode characters\n",
    "    * Emoticons are less varied, and don’t have as many nouns\n",
    "\n",
    "### Identify features of text on twitter or other social media platforms that make it distinct from regular published text (e.g. news).\n",
    "* Emoji, Emoticons, Hashtags, Addressing (using @),\n",
    "* Acronym = lol, idk\n",
    "* Textspeak (non-acronym) shorter words involves orthographic variants based on the phonetic properties of a word (creative spelling)\n",
    "    * EG: Gr8, 2day, b4\n",
    "* Misspellings (non-textspeak)\n",
    "    * Elongations \n",
    "        * EG: Soooooo \n",
    "    * Clippings/shorternings\n",
    "        * EG: Tho, kno, cuz\n",
    "    * Conjoined: words squished together without a space\n",
    "        * E.g. #thisiswhativewaitedfor\n",
    "\n",
    "### Discuss how these sort of features can result in errors in NLP systems if special steps are not taken.\n",
    "* For misspellings, like elongations, if it’s not converted back to a more standard form, the parser can miss important information. \n",
    "    * EG: A word like “soooooo long” would be OOV since “soooooooo” isn’t in the vocab. \n",
    "\n",
    "### Provide possible solutions for problems stemming from the special features of social media.\n",
    "* Packages like ekphrasis can annotate special features, like “elongated”, “hashtag”, “repeated”, to help encode these additional information\n",
    "* Bigram / trigrams could help give more context around special features\n",
    "* Spelling correctors, but it would have to be very advanced and cover a wide corpus of misspellings. \n",
    "\n",
    "### Understand why, in the context of sentiment analysis or author profiling, why it might be a bad idea to simply remove the features that cause trouble.\n",
    "\n",
    "* Removing these important features would mean removing certain amplifications.\n",
    "    * EG: For example “sooooo long”, if we remove “soooooo” - then we lose the amplification, which can change sentiment from very negative to negative\n",
    "    * Cool vs Coooooooool (they should be different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
